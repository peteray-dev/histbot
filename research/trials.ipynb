{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Desktop\\\\History-and-philosophy-chatbot\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Desktop\\\\History-and-philosophy-chatbot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of Document\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data, glob=\"*.pdf\", loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data=\"Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1210 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(extracted_data)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Data\\\\Ayomide_Popoola_CV_AI_ML_Engineer.pdf', 'page': 0, 'page_label': '1'}, page_content='POPOOLA, AYOMIDE PETER, MBCS \\nTel: +44 7586 367163 Email Address: popoolaayomide37@gmail.com  \\nLinkedIn profile: https://www.linkedin.com/in/ayomide-popoola-60092a12a/  \\nPortfolio: https://peteray-dev.github.io/Portfolio_DS/   GitHub profile: https://github.com/peteray-dev/ \\n \\nPROFESSIONAL SUMMARY software engineer compliance ml  \\nInnovative AI & Machine Learning Engineer with expertise in developing, deploying, and optimising machine \\nlearning models to drive measurable business impact. Proficient in generative AI, deep learning, natural language \\nprocessing (NLP), large language models (LLMs), and high -performance computing (HPC). Adept at \\nimplementing MLOps best practices, CI/CD pipelines, and cloud -based AI solutions to enhance operational \\nefficiency. Passionate about collaborating with stakeholders to deliver innovative, data-driven products that elevate \\ncustomer experience. \\nEDUCATION  \\n• M.Sc. Data Science (Distinction) \\nManchester Metropolitan University, Manchester, United Kingdom \\n• Data Management and Governance, Exploratory data analysis, Computational Mathematics and \\nStatistics, Algorithms and Data Structures, Machine learning, Deep learning (Pytorch, NLP), High \\nPerformance Computing, Big Data Technologies (PySpark) and Version Control (Git). \\n• B.Sc. Biochemistry (First Class Honours) \\nUniversity of Ilorin, Ilorin, Nigeria, 2019 \\n \\nWORK EXPERIENCES  \\nData Scientist/Tutor \\nCambridge Spark Limited (Lloyds Bank Group) United Kingdom, Sept 2024 – Dec 2024 \\n• Built and deployed AI/ML models, improving fraud detection accuracy to 92% and reducing false \\npositives. \\n• Trained graduate interns, leading to a 95% project completion rate. \\n• Designed and implemented MLOps pipelines, automating model deployment, monitoring, and updates. \\n• Optimised ML workflows, reducing model training time by 40%. \\n• Collaborated with cross-functional teams to align ML solutions with business objectives. \\nProject Data Analyst \\nFuncast Consulting and Management Limited, Jun 2021 - Sept 2024 \\n• Conducted data analysis to provide insights that improved project execution efficiency by 65%. \\n• Developed risk analysis models, boosting company profitability by 36%. \\n• Implemented predictive analytics, improving project delivery timelines by 30%. \\n• Worked with stakeholders to design data-driven strategies that enhanced operational decision-making. \\n \\nSoftware Developer (MERN) \\nAlabian Solutions Limited, Nov 2020 - May 2021 ')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~Chunking\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunk = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_split(extracted_data=extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_30584\\786653829.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"hello world\")\n",
    "\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "# from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"histphilbot\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "doc_search = PineconeVectorStore.from_documents(\n",
    "    documents = text_chunks,\n",
    "    index_name= index_name,\n",
    "    embedding= embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "doc_search = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = doc_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc = retriever.invoke(\"who is popoola?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2112917b-c03b-4570-a130-7418733d0c5e', metadata={'page': 363.0, 'page_label': '364', 'source': 'Data\\\\World_History-book.pdf'}, page_content='When I was at Marcavillca, in the province of Xauxa, I asked the lord Guacarapora to explain it in such a\\nway as that my mind might be satisfied, and that I might be assured that it was true and accurate. He\\nordered his servants to bring the quipus, and as this lord was a native, and a man of good\\nunderstanding, he proceeded to make the thing clear to me. He told me to observe that all that he, for'),\n",
       " Document(id='a53f3ec5-67ac-4843-9dae-12b5fbd504ea', metadata={'page': 43.0, 'page_label': '44', 'source': 'Data\\\\phylosophy-book.pdf'}, page_content='podcast “Philosophy Bites” that is one of the most downloaded podcasts on academic topics. He also is an\\neditor-in-chief of the online magazine Aeon. David Barnett, a former philosophy professor, founded the\\ncompany PopSockets in 2012 after leaving academia. That company now employs over 200 people and\\ngenerates hundreds of millions of dollars in annual revenue. Additionally, there are a growing number of'),\n",
       " Document(id='9c8718fa-4a5b-49e3-9e24-e7c471c1c70d', metadata={'page': 0.0, 'page_label': '1', 'source': 'Data\\\\Ayomide_Popoola_CV_AI_ML_Engineer.pdf'}, page_content='POPOOLA, AYOMIDE PETER, MBCS \\nTel: +44 7586 367163 Email Address: popoolaayomide37@gmail.com  \\nLinkedIn profile: https://www.linkedin.com/in/ayomide-popoola-60092a12a/  \\nPortfolio: https://peteray-dev.github.io/Portfolio_DS/   GitHub profile: https://github.com/peteray-dev/ \\n \\nPROFESSIONAL SUMMARY software engineer compliance ml  \\nInnovative AI & Machine Learning Engineer with expertise in developing, deploying, and optimising machine')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have a complete reply, i will integrate such that the query get the similarities\n",
    "# fromm the knowledge based and then hits the llm (openai) along side the query itself\n",
    "# the llm then process the 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(OPENAI_API_KEY )\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know, and it is out of context. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Popoola is a software engineer with expertise in developing, deploying, and optimizing machine learning models. He has a background in philosophy and has worked in academia before founding a successful company in 2012. He is also an editor-in-chief of an online magazine and has a popular podcast on academic topics. \n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"who is popoola?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
